"use client";

import { useState, useEffect, useRef, useCallback } from "react";
import { videoChatAPI } from "../api/videoChat";
import type { Participant, WebRTCMessage } from "@/shared/types/video";



export function useVideoChat() {
    const [showParticipants, setShowParticipants] = useState(true);
    const [chatWidth, setChatWidth] = useState(320);
    const [message, setMessage] = useState("");
    const [messages, setMessages] = useState<{ name: string; text: string }[]>([]);
    const [participants, setParticipants] = useState<Participant[]>([]);
    const [remoteStreams, setRemoteStreams] = useState<{ [userId: string]: MediaStream }>({});
    const [selectedParticipant, setSelectedParticipant] = useState<string | null>(null);
    const [roomId, setRoomId] = useState<string>("");
    const [isConnected, setIsConnected] = useState(false);
    const [connectionStatus, setConnectionStatus] = useState("Not connected");

    const chatScrollRef = useRef<HTMLDivElement>(null);
    const videoRef = useRef<HTMLVideoElement>(null);
    const wsRef = useRef<WebSocket | null>(null);
    const localStreamRef = useRef<MediaStream | null>(null);
    const screenStreamRef = useRef<MediaStream | null>(null);
    const peerConnectionsRef = useRef<{ [userId: string]: RTCPeerConnection }>({});
    const remoteVideosRef = useRef<{ [userId: string]: HTMLVideoElement }>({});
    const heartbeatIntervalRef = useRef<NodeJS.Timeout | null>(null);
    const currentSessionIdRef = useRef<string>("");
    const mediaRecorderRef = useRef<MediaRecorder | null>(null);
    const recordedChunksRef = useRef<Blob[]>([]);
    const audioContextRef = useRef<AudioContext | null>(null);
    const mixedStreamRef = useRef<MediaStream | null>(null);
    const joinSoundRef = useRef<HTMLAudioElement | null>(null);
    const leaveSoundRef = useRef<HTMLAudioElement | null>(null);
    const audioRecordersRef = useRef<{ [userId: string]: { recorder: MediaRecorder; intervalId: NodeJS.Timeout } }>({});
    
    // ì˜¤ë””ì˜¤ WebSocket ê´€ë ¨ refs
    const audioWsRef = useRef<WebSocket | null>(null);
    const audioSeqRef = useRef<number>(0);
    const audioMediaRecorderRef = useRef<MediaRecorder | null>(null);
    const audioHeaderRef = useRef<Uint8Array | null>(null); // WebM/Ogg í—¤ë” ìºì‹œ

    const [isScreenSharing, setIsScreenSharing] = useState(false);
    const [isRecording, setIsRecording] = useState(false);
    const [separatedAudioData, setSeparatedAudioData] = useState<{[key: string]: {src: string, data: string}[]}>({});

    const pc_config: RTCConfiguration = { 
        iceServers: [
            { urls: "stun:stun.l.google.com:19302" },
            { urls: "stun:stun1.l.google.com:19302" },
            { urls: "stun:stun2.l.google.com:19302" },
            { 
                urls: [
                    "turn:openrelay.metered.ca:80",
                    "turn:openrelay.metered.ca:443",
                    "turn:openrelay.metered.ca:443?transport=tcp"
                ],
                username: "openrelayproject",
                credential: "openrelayproject"
            },
            {
                urls: [
                    "turn:relay1.expressturn.com:3478",
                    "turns:relay1.expressturn.com:5349"
                ],
                username: "efSW8RR1XTQSTE6K33",
                credential: "Uc4ZYmfJMmfYHVzx"
            }
        ],
        iceCandidatePoolSize: 10,
        iceTransportPolicy: "all"
    };

    // íš¨ê³¼ìŒ ì´ˆê¸°í™”
    useEffect(() => {
        try {
            console.log('Initializing sound effects...');
            joinSoundRef.current = new Audio(encodeURI('/assets/sound/sound.mp3'));
            leaveSoundRef.current = new Audio(encodeURI('/assets/sound/sound1.mp3'));
            
            // ë³¼ë¥¨ ì„¤ì •
            if (joinSoundRef.current) {
                joinSoundRef.current.volume = 0.5;
                console.log('Join sound initialized');
            }
            if (leaveSoundRef.current) {
                leaveSoundRef.current.volume = 0.5;
                console.log('Leave sound initialized');
            }
            
            // ì˜¤ë””ì˜¤ ë¡œë“œ ì—ëŸ¬ í•¸ë“¤ë§
            if (joinSoundRef.current) {
                joinSoundRef.current.onerror = (e) => {
                    console.error('Failed to load join sound:', e);
                };
            }
            if (leaveSoundRef.current) {
                leaveSoundRef.current.onerror = (e) => {
                    console.error('Failed to load leave sound:', e);
                };
            }
        } catch (error) {
            console.error('Error initializing sound effects:', error);
        }
        
        return () => {
            // ì •ë¦¬
            if (joinSoundRef.current) {
                joinSoundRef.current.pause();
                joinSoundRef.current = null;
            }
            if (leaveSoundRef.current) {
                leaveSoundRef.current.pause();
                leaveSoundRef.current = null;
            }
        };
    }, []);

    useEffect(() => {
        if (chatScrollRef.current) {
            chatScrollRef.current.scrollTop = chatScrollRef.current.scrollHeight;
        }
    }, [messages]);

    const sendWebSocketMessage = useCallback((message: any) => {
        if (wsRef.current && wsRef.current.readyState === WebSocket.OPEN) {
            wsRef.current.send(JSON.stringify(message));
        }
    }, []);

    const playJoinSound = () => {
        if (joinSoundRef.current) {
            console.log('Attempting to play join sound');
            joinSoundRef.current.currentTime = 0;
            joinSoundRef.current.play()
                .then(() => console.log('Join sound played successfully'))
                .catch(error => {
                    console.error('Failed to play join sound:', error);
                });
        } else {
            console.warn('Join sound ref is null');
        }
    };

    const playLeaveSound = () => {
        if (leaveSoundRef.current) {
            console.log('Attempting to play leave sound');
            leaveSoundRef.current.currentTime = 0;
            leaveSoundRef.current.play()
                .then(() => console.log('Leave sound played successfully'))
                .catch(error => {
                    console.error('Failed to play leave sound:', error);
                });
        } else {
            console.warn('Leave sound ref is null');
        }
    };

    const createRoom = async (title: string, teamId: number = 1, maxParticipants: number = 20) => {
        try {
            const result = await videoChatAPI.createRoom({ title, teamId, maxParticipants });
            if (result.roomId) {
                setRoomId(result.roomId);
                return result.roomId;
            }
            throw new Error('Failed to create room');
        } catch (error) {
            console.error('Error creating room:', error);
            throw error;
        }
    };

    const startLocalMedia = async () => {
        try {
            const stream = await navigator.mediaDevices.getUserMedia({ video: true, audio: true });
            localStreamRef.current = stream;
            
            // ë¡œì»¬ ìŠ¤íŠ¸ë¦¼ íŠ¸ë™ í™•ì¸
            const audioTracks = stream.getAudioTracks();
            const videoTracks = stream.getVideoTracks();
            console.log('Local stream tracks:', {
                audioTracks: audioTracks.length,
                videoTracks: videoTracks.length,
                audioEnabled: audioTracks[0]?.enabled,
                videoEnabled: videoTracks[0]?.enabled,
                audioLabel: audioTracks[0]?.label,
                videoLabel: videoTracks[0]?.label
            });
            
            if (videoRef.current) {
                videoRef.current.srcObject = stream;
            }
            return stream;
        } catch (error) {
            console.error('Error accessing media devices:', error);
            throw error;
        }
    };

    const createPeerConnection = useCallback((userId: string, isOfferor: boolean) => {
        if (peerConnectionsRef.current[userId]) return;

        const pc = new RTCPeerConnection(pc_config);
        peerConnectionsRef.current[userId] = pc;

        // ICE ì—°ê²° ìƒíƒœ ë¡œê¹…
        pc.oniceconnectionstatechange = () => {
            console.log(`ICE connection state for ${userId}:`, pc.iceConnectionState);
            if (pc.iceConnectionState === 'failed') {
                console.error(`ICE connection failed for ${userId}. May need TURN server.`);
            }
        };

        // ì—°ê²° ìƒíƒœ ë¡œê¹…
        pc.onconnectionstatechange = () => {
            console.log(`Connection state for ${userId}:`, pc.connectionState);
        };

        // ICE gathering ìƒíƒœ ë¡œê¹…
        pc.onicegatheringstatechange = () => {
            console.log(`ICE gathering state for ${userId}:`, pc.iceGatheringState);
        };

        // Signaling ìƒíƒœ ë¡œê¹…
        pc.onsignalingstatechange = () => {
            console.log(`Signaling state for ${userId}:`, pc.signalingState);
        };

        pc.onicecandidate = (event) => {
            if (event.candidate) {
                console.log(`Sending ICE candidate to ${userId}`);
                sendWebSocketMessage({ type: 'candidate', to: userId, candidate: event.candidate });
            }
        };

        pc.ontrack = (event) => {
            console.log(`ontrack event for ${userId}:`, {
                kind: event.track.kind,
                enabled: event.track.enabled,
                muted: event.track.muted,
                readyState: event.track.readyState
            });
            
            // ì´ë¯¸ participantsì— ìˆëŠ” ê²½ìš° userName ê°€ì ¸ì˜¤ê¸°
            const existingParticipant = participants.find(p => p.id === userId);
            addRemoteVideo(userId, event.streams[0], existingParticipant?.name);
        };

        if (localStreamRef.current) {
            localStreamRef.current.getTracks().forEach(track => {
                const sender = pc.addTrack(track, localStreamRef.current!);
                console.log(`Added ${track.kind} track to peer connection for ${userId}`);
            });
            
            // Sender í™•ì¸
            const senders = pc.getSenders();
            console.log(`Senders for ${userId}:`, senders.map(s => s.track?.kind));
        }

        if (isOfferor) {
            pc.createOffer({ offerToReceiveAudio: true, offerToReceiveVideo: true })
                .then(offer => {
                    console.log(`Offer created for ${userId}, SDP:`, offer.sdp?.substring(0, 200));
                    return pc.setLocalDescription(offer);
                })
                .then(() => {
                    console.log(`Local description set for ${userId}, has m=audio:`, pc.localDescription?.sdp?.includes('m=audio'));
                    sendWebSocketMessage({ type: 'offer', to: userId, sdp: pc.localDescription });
                })
                .catch(e => console.error(`Offer creation failed for ${userId}:`, e));
        }
    }, [sendWebSocketMessage, participants]);

    const addRemoteVideo = (userId: string, stream: MediaStream, userName?: string) => {
        setParticipants(prev => {
            if (!prev.find(p => p.id === userId)) {
                return [...prev, { id: userId, name: userName || `User ${userId.substring(0, 8)}` }];
            }
            return prev;
        });

        setRemoteStreams(prev => ({
            ...prev,
            [userId]: stream
        }));

        const audioTracks = stream.getAudioTracks();
        const videoTracks = stream.getVideoTracks();
        console.log(`Remote stream added for user ${userId} (${userName || 'Unknown'}):`, {
            audioTracks: audioTracks.length,
            videoTracks: videoTracks.length,
            audioEnabled: audioTracks[0]?.enabled,
            videoEnabled: videoTracks[0]?.enabled
        });

        // ë…¹ìŒ ì¤‘ì´ë©´ ìƒˆë¡œìš´ ì˜¤ë””ì˜¤ë¥¼ ë¯¹ì‹±ì— ì¶”ê°€
        if (isRecording && audioContextRef.current && audioTracks.length > 0) {
            try {
                const destination = audioContextRef.current.createMediaStreamDestination();
                const remoteSource = audioContextRef.current.createMediaStreamSource(
                    new MediaStream([audioTracks[0]])
                );
                remoteSource.connect(destination);
                console.log(`Added new remote audio from ${userId} to ongoing recording`);
            } catch (error) {
                console.error('Failed to add new audio to recording:', error);
            }
        }
        
        // ë°±ì—”ë“œ STT: ì›ê²© ì˜¤ë””ì˜¤ ìº¡ì²˜ ì‹œì‘
        if (audioTracks.length > 0 && userName && roomId) {
            startAudioCapture(userId, userName, stream, roomId);
        }
    };

    const closePeerConnection = (userId: string) => {
        if (peerConnectionsRef.current[userId]) {
            peerConnectionsRef.current[userId].close();
            delete peerConnectionsRef.current[userId];
        }

        setParticipants(prev => prev.filter(p => p.id !== userId));

        setRemoteStreams(prev => {
            const newStreams = { ...prev };
            delete newStreams[userId];
            return newStreams;
        });

        setSelectedParticipant(prev => prev === userId ? null : prev);

        if (remoteVideosRef.current[userId]) {
            delete remoteVideosRef.current[userId];
        }
    };

    const joinRoom = async (roomId: string) => {
        try {
            setConnectionStatus(`Connecting to room ${roomId}...`);
            console.log('Attempting to join room:', roomId);

            await startLocalMedia();

            // ì‚¬ìš©ì ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            const { getUserInfo } = await import('../api/user');
            const userInfo = await getUserInfo();
            console.log('User info for WebSocket:', { userId: userInfo.id, userName: userInfo.name });

            // í™”ìƒí†µí™”ìš© Spring API URL ì‚¬ìš©
            const { getApiBaseUrl } = await import('../lib/envCheck');
            const apiBaseUrl = getApiBaseUrl();
            console.log('Using Spring API Base URL for video chat:', apiBaseUrl);
            
            const wsProtocol = apiBaseUrl.startsWith('https') ? 'wss' : 'ws';
            const wsHost = apiBaseUrl.replace(/^https?:\/\//, '');
            
            // WebSocket URLì— roomId, userId, userName íŒŒë¼ë¯¸í„° ì¶”ê°€
            const wsUrl = `${wsProtocol}://${wsHost}/api/ws/signal?roomId=${roomId}&userId=${userInfo.id}&userName=${encodeURIComponent(userInfo.name)}`;
            
            console.log('WebSocket URL:', wsUrl);
            
            const ws = new WebSocket(wsUrl);
            wsRef.current = ws;

            ws.onopen = () => {
                setIsConnected(true);
                setConnectionStatus(`Connected to room ${roomId}`);
                console.log('WebSocket connected successfully');
                
                // ì…ì¥ íš¨ê³¼ìŒ ì¬ìƒ
                playJoinSound();
                
                // ë°±ì—”ë“œ STT: ë¡œì»¬ ì˜¤ë””ì˜¤ ìº¡ì²˜ ì‹œì‘
                setTimeout(() => {
                    if (localStreamRef.current) {
                        startAudioCapture(userInfo.id.toString(), userInfo.name, localStreamRef.current, roomId);
                    }
                }, 1500);
                
                // Heartbeat: 30ì´ˆë§ˆë‹¤ ping ë©”ì‹œì§€ ì „ì†¡
                heartbeatIntervalRef.current = setInterval(() => {
                    if (ws.readyState === WebSocket.OPEN) {
                        ws.send(JSON.stringify({ type: 'ping' }));
                        console.log('Sent ping to keep connection alive');
                    }
                }, 30000); // 30ì´ˆ
            };

            ws.onmessage = async (event) => {
                const message: WebRTCMessage = JSON.parse(event.data);
                const from = message.from;
                const data = message.data;

                console.log('WebSocket message received:', message.type, message);

                // to í•„ë“œê°€ ìˆëŠ” ë©”ì‹œì§€ëŠ” í•´ë‹¹ ì‚¬ìš©ìë§Œ ì²˜ë¦¬
                // ë‹¨, ë‚´ ì„¸ì…˜ IDë¥¼ ì•„ì§ ëª¨ë¥´ë©´ ì¼ë‹¨ ì²˜ë¦¬ (ì²« offerì—ì„œ ì„¸ì…˜ ID ì¶”ë¡ )
                if (message.to && currentSessionIdRef.current && message.to !== currentSessionIdRef.current) {
                    console.log('Message not for me, ignoring');
                    return;
                }

                switch (message.type) {
                    case 'session_id':
                        // ì„œë²„ì—ì„œ ë‚´ ì„¸ì…˜ IDë¥¼ ì•Œë ¤ì¤Œ
                        if (message.sessionId) {
                            currentSessionIdRef.current = message.sessionId;
                            console.log('My session ID from server:', currentSessionIdRef.current);
                        }
                        break;
                    case 'existing_users':
                        const existingUsers = message.users || [];
                        setConnectionStatus(`Found ${existingUsers.length} existing users. Connecting...`);
                        
                        // ë‚´ ì„¸ì…˜ ID ì €ì¥ (dataì— ë‚´ ì„¸ì…˜ IDê°€ í¬í•¨ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŒ)
                        if (message.data && typeof message.data === 'string' && !currentSessionIdRef.current) {
                            currentSessionIdRef.current = message.data;
                            console.log('My session ID:', currentSessionIdRef.current);
                        }
                        
                        for (const userInfo of existingUsers) {
                            const userId = userInfo.sessionId;
                            const userName = userInfo.userName;
                            createPeerConnection(userId, true);
                            // ì‚¬ìš©ì ì •ë³´ë¥¼ ë¯¸ë¦¬ ì €ì¥
                            setParticipants(prev => {
                                if (!prev.find(p => p.id === userId)) {
                                    return [...prev, { id: userId, name: userName }];
                                }
                                return prev;
                            });
                        }
                        break;
                    case 'new_user':
                        const newUserId = message.user?.sessionId || data;
                        const newUserName = message.user?.userName;
                        setConnectionStatus(`User ${newUserName || newUserId} joined. Preparing connection...`);
                        
                        // ë‹¤ë¥¸ ì‚¬ëŒì´ ì…ì¥í–ˆì„ ë•Œ íš¨ê³¼ìŒ ì¬ìƒ
                        playJoinSound();
                        
                        createPeerConnection(newUserId, false);
                        // ì‚¬ìš©ì ì •ë³´ë¥¼ ë¯¸ë¦¬ ì €ì¥
                        if (newUserName) {
                            setParticipants(prev => {
                                if (!prev.find(p => p.id === newUserId)) {
                                    return [...prev, { id: newUserId, name: newUserName }];
                                }
                                return prev;
                            });
                        }
                        break;
                    case 'offer':
                        console.log(`Received offer from ${from}, SDP:`, message.sdp?.sdp?.substring(0, 200));
                        console.log(`Offer has m=audio:`, message.sdp?.sdp?.includes('m=audio'));
                        
                        // offerë¥¼ ë°›ì•˜ë‹¤ëŠ” ê²ƒì€ toê°€ ë‚´ ì„¸ì…˜ ID
                        if (message.to && !currentSessionIdRef.current) {
                            currentSessionIdRef.current = message.to;
                            console.log('My session ID from offer:', currentSessionIdRef.current);
                        }
                        
                        if (!peerConnectionsRef.current[from!]) {
                            createPeerConnection(from!, false);
                        }
                        
                        await peerConnectionsRef.current[from!].setRemoteDescription(new RTCSessionDescription(message.sdp!));
                        console.log(`Remote description set for ${from}`);
                        
                        const answer = await peerConnectionsRef.current[from!].createAnswer();
                        console.log(`Answer created for ${from}, has m=audio:`, answer.sdp?.includes('m=audio'));
                        
                        await peerConnectionsRef.current[from!].setLocalDescription(answer);
                        console.log(`Sending answer to ${from}`);
                        
                        sendWebSocketMessage({ type: 'answer', to: from, sdp: answer });
                        break;
                    case 'answer':
                        console.log(`Received answer from ${from}, SDP:`, message.sdp?.sdp?.substring(0, 200));
                        console.log(`Answer has m=audio:`, message.sdp?.sdp?.includes('m=audio'));
                        
                        await peerConnectionsRef.current[from!].setRemoteDescription(new RTCSessionDescription(message.sdp!));
                        console.log(`Remote description (answer) set for ${from}`);
                        break;
                    case 'candidate':
                        if (peerConnectionsRef.current[from!]) {
                            try {
                                await peerConnectionsRef.current[from!].addIceCandidate(new RTCIceCandidate(message.candidate!));
                                console.log(`ICE candidate added for ${from}`);
                            } catch (e) {
                                console.error(`Error adding ICE candidate for ${from}:`, e);
                            }
                        } else {
                            console.warn(`Received candidate for ${from} but no peer connection exists`);
                        }
                        break;
                    case 'user_left':
                        // dataê°€ ê°ì²´ì¸ ê²½ìš° sessionId ì¶”ì¶œ
                        const leftUserId = typeof data === 'object' && data !== null ? data.sessionId : data;
                        const leftUserName = typeof data === 'object' && data !== null ? data.userName : '';
                        setConnectionStatus(`User ${leftUserName || leftUserId} left.`);
                        
                        // ë‹¤ë¥¸ ì‚¬ëŒì´ í‡´ì¥í–ˆì„ ë•Œ íš¨ê³¼ìŒ ì¬ìƒ
                        playLeaveSound();
                        
                        closePeerConnection(leftUserId);
                        break;
                    case 'chat':
                    case 'chat_message':
                        // ì±„íŒ… ë©”ì‹œì§€ ìˆ˜ì‹  (ì„œë²„ê°€ ë¸Œë¡œë“œìºìŠ¤íŠ¸í•˜ë¯€ë¡œ ëª¨ë“  ë©”ì‹œì§€ í‘œì‹œ)
                        if (message.message && message.user?.userName) {
                            setMessages(prev => [...prev, { 
                                name: message.user!.userName, 
                                text: message.message! 
                            }]);
                        }
                        break;
                    case 'stt':
                        // STT ê²°ê³¼ ìˆ˜ì‹  (ë‹¤ë¥¸ ì°¸ê°€ìì˜ ìŒì„± ì¸ì‹ ê²°ê³¼)
                        const sttMessage = message as any;
                        if (sttMessage.userName && sttMessage.transcript) {
                            console.log('ğŸ“¥ Received STT:', sttMessage.userName, '-', sttMessage.transcript);
                            console.log(`ğŸ—£ï¸ ${sttMessage.userName}: ${sttMessage.transcript}`);
                            // í•„ìš”ì‹œ UIì— í‘œì‹œí•˜ê±°ë‚˜ íšŒì˜ë¡ì— ì¶”ê°€
                        }
                        break;
                    case 'error':
                        console.error(`Error from server: ${message.message}`);
                        alert(`í™”ìƒí†µí™” ì˜¤ë¥˜: ${message.message}`);
                        leaveRoom();
                        break;
                }
            };

            ws.onclose = (event) => {
                setIsConnected(false);
                setConnectionStatus('Disconnected');
                console.log('WebSocket disconnected', { code: event.code, reason: event.reason });
                
                // Heartbeat ì •ë¦¬
                if (heartbeatIntervalRef.current) {
                    clearInterval(heartbeatIntervalRef.current);
                    heartbeatIntervalRef.current = null;
                }
                
                stopLocalMedia();
            };

            ws.onerror = (error) => {
                console.error('WebSocket Error:', error);
                setConnectionStatus('Connection error');
            };

        } catch (error) {
            console.error('Error joining room:', error);
            setConnectionStatus('Failed to join room');
            alert('í™”ìƒí†µí™” ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.');
        }
    };

    const leaveRoom = async () => {
        // í‡´ì¥ íš¨ê³¼ìŒ ì¬ìƒ
        playLeaveSound();
        
        // ë…¹ìŒ ì¤‘ì§€ (ìë™ìœ¼ë¡œ ì—…ë¡œë“œë¨)
        if (isRecording) {
            stopRecording();
            console.log('Auto-stopped recording on leave');
        }
        
        // ì˜¤ë””ì˜¤ WebSocket ì •ë¦¬
        stopAudioWebSocket();

        try {
            if (roomId) {
                await videoChatAPI.leaveRoom(roomId);
            }
        } catch (error) {
            console.error('Error leaving room via API:', error);
        }

        // Heartbeat ì •ë¦¬
        if (heartbeatIntervalRef.current) {
            clearInterval(heartbeatIntervalRef.current);
            heartbeatIntervalRef.current = null;
        }

        if (wsRef.current) {
            wsRef.current.close();
        }
        stopLocalMedia();
        setIsConnected(false);
        setParticipants([]);
        setRoomId("");
    };

    const listRooms = async () => {
        try {
            return await videoChatAPI.listRooms();
        } catch (error) {
            console.error('Error listing rooms:', error);
            throw error;
        }
    };

    const findOrCreateTeamRoom = async (teamId: number) => {
        try {
            const room = await videoChatAPI.findOrCreateTeamRoom(teamId);
            console.log('Team room:', room);
            setRoomId(room.roomId);
            return room;
        } catch (error) {
            console.error('Error finding or creating team room:', error);
            throw error;
        }
    };

    const stopLocalMedia = () => {
        if (localStreamRef.current) {
            localStreamRef.current.getTracks().forEach(track => track.stop());
            localStreamRef.current = null;
        }
        if (screenStreamRef.current) {
            screenStreamRef.current.getTracks().forEach(track => track.stop());
            screenStreamRef.current = null;
        }
        if (videoRef.current) {
            videoRef.current.srcObject = null;
        }

        Object.keys(peerConnectionsRef.current).forEach(userId => {
            closePeerConnection(userId);
        });

        setIsScreenSharing(false);
    };

    const toggleCamera = () => {
        if (!localStreamRef.current) return;
        const videoTrack = localStreamRef.current.getVideoTracks()[0];
        if (videoTrack) {
            videoTrack.enabled = !videoTrack.enabled;
        }
    };

    const toggleMicrophone = () => {
        if (!localStreamRef.current) return;
        const audioTrack = localStreamRef.current.getAudioTracks()[0];
        if (audioTrack) {
            audioTrack.enabled = !audioTrack.enabled;
            console.log('ğŸ¤ Microphone:', audioTrack.enabled ? 'ON' : 'OFF');
            
            // ë§ˆì´í¬ êº¼ì§€ë©´ ë…¹ìŒë„ ì¤‘ì§€
            if (!audioTrack.enabled) {
                Object.entries(audioRecordersRef.current).forEach(([userId, { recorder, intervalId }]) => {
                    if (recorder.state === 'recording') {
                        recorder.stop();
                        clearInterval(intervalId);
                        console.log('â¸ï¸ Paused recording for', userId);
                    }
                });
            } else {
                // ë§ˆì´í¬ ì¼œì§€ë©´ ë…¹ìŒ ì¬ê°œ
                Object.entries(audioRecordersRef.current).forEach(([userId, { recorder, intervalId }]) => {
                    if (recorder.state === 'inactive') {
                        recorder.start();
                        console.log('â–¶ï¸ Resumed recording for', userId);
                    }
                });
            }
        }
    };

    const startScreenShare = async () => {
        try {
            const screenStream = await navigator.mediaDevices.getDisplayMedia({ video: true });
            screenStreamRef.current = screenStream;
            const screenTrack = screenStream.getVideoTracks()[0];

            // Replace video track in all peer connections (if any)
            for (const userId in peerConnectionsRef.current) {
                const pc = peerConnectionsRef.current[userId];
                const videoSender = pc.getSenders().find(sender => sender.track?.kind === 'video');
                if (videoSender) {
                    await videoSender.replaceTrack(screenTrack);
                }
            }

            // ë¡œì»¬ ë¹„ë””ì˜¤ë„ í™”ë©´ ê³µìœ ë¡œ ë³€ê²½
            if (videoRef.current) {
                videoRef.current.srcObject = screenStream;
            }

            setIsScreenSharing(true);

            screenTrack.onended = () => {
                stopScreenShare();
            };
        } catch (error) {
            console.error('Screen share failed:', error);
            throw error;
        }
    };

    const stopScreenShare = async () => {
        if (!localStreamRef.current) return;

        const cameraTrack = localStreamRef.current.getVideoTracks()[0];

        // ëª¨ë“  í”¼ì–´ ì—°ê²°ì˜ ë¹„ë””ì˜¤ íŠ¸ë™ì„ ì¹´ë©”ë¼ë¡œ ë³µì›
        for (const userId in peerConnectionsRef.current) {
            const pc = peerConnectionsRef.current[userId];
            const videoSender = pc.getSenders().find(sender => sender.track?.kind === 'video');
            if (videoSender && cameraTrack) {
                await videoSender.replaceTrack(cameraTrack);
            }
        }

        // ë¡œì»¬ ë¹„ë””ì˜¤ë„ ì¹´ë©”ë¼ë¡œ ë³µì›
        if (videoRef.current && localStreamRef.current) {
            videoRef.current.srcObject = localStreamRef.current;
        }

        // í™”ë©´ ê³µìœ  ìŠ¤íŠ¸ë¦¼ ì •ë¦¬
        if (screenStreamRef.current) {
            screenStreamRef.current.getTracks().forEach(track => track.stop());
            screenStreamRef.current = null;
        }

        setIsScreenSharing(false);
    };

    const startAudioWebSocket = async (roomId: string, userId: number, userName: string) => {
        try {
            const { getApiBaseUrl } = await import('../lib/envCheck');
            const apiBaseUrl = getApiBaseUrl();
            const wsProtocol = apiBaseUrl.startsWith('https') ? 'wss' : 'ws';
            const wsHost = apiBaseUrl.replace(/^https?:\/\//, '');
            
            const audioWsUrl = `${wsProtocol}://${wsHost}/api/ws/audio?roomId=${roomId}&userId=${userId}&userName=${encodeURIComponent(userName)}`;
            console.log('Connecting to audio WebSocket (Spring):', audioWsUrl);
            console.log('roomId for audio recording:', roomId, ', userName:', userName);
            
            const audioWs = new WebSocket(audioWsUrl);
            audioWsRef.current = audioWs;
            audioSeqRef.current = 0;
            
            audioWs.onopen = () => {
                console.log('Audio WebSocket connected');
                // MediaRecorder ì‹œì‘ (1ì´ˆ ì²­í¬) - roomId, userId, userName ì „ë‹¬
                startAudioRecording(roomId, userId, userName);
            };
            
            audioWs.onmessage = (event) => {
                try {
                    const message = JSON.parse(event.data);
                    console.log('Audio WebSocket message:', message.type);
                    
                    if (message.type === 'separated_audio') {
                        handleSeparatedAudio(message);
                    } else if (message.type === 'error') {
                        console.error('Audio WebSocket error:', message.message);
                        alert(`ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì˜¤ë¥˜: ${message.message}`);
                        stopAudioWebSocket();
                    }
                } catch (error) {
                    console.error('Failed to parse audio WebSocket message:', error);
                }
            };
            
            audioWs.onclose = () => {
                console.log('Audio WebSocket disconnected');
                stopAudioRecording();
            };
            
            audioWs.onerror = (error) => {
                console.error('Audio WebSocket error:', error);
                stopAudioRecording();
            };
        } catch (error) {
            console.error('Failed to start audio WebSocket:', error);
        }
    };
    
    const stopAudioWebSocket = () => {
        if (audioWsRef.current) {
            audioWsRef.current.close();
            audioWsRef.current = null;
        }
        stopAudioRecording();
        audioSeqRef.current = 0;
    };
    
    const startAudioRecording = (roomId: string, userId: number, userName: string) => {
        try {
            if (!localStreamRef.current) {
                console.warn('No local stream available for audio recording');
                return;
            }
            
            const audioTrack = localStreamRef.current.getAudioTracks()[0];
            if (!audioTrack) {
                console.warn('No audio track available');
                return;
            }
            
            // roomId ê²€ì¦ (ë…¹ìŒ ì‹œì‘ ì „)
            if (!roomId) {
                console.error('roomId is empty! Cannot start audio recording.');
                return;
            }
            
            console.log('Starting audio recording with roomId:', roomId, ', userId:', userId, ', userName:', userName);
            
            // ì˜¤ë””ì˜¤ íŠ¸ë™ ìƒíƒœ í™•ì¸
            console.log('Audio track info:', {
                label: audioTrack.label,
                enabled: audioTrack.enabled,
                muted: audioTrack.muted,
                readyState: audioTrack.readyState
            });
            
            const audioStream = new MediaStream([audioTrack]);
            
            // ë¸Œë¼ìš°ì €ê°€ ì§€ì›í•˜ëŠ” mimeType í™•ì¸
            const mimeType = MediaRecorder.isTypeSupported('audio/webm;codecs=opus')
                ? 'audio/webm;codecs=opus'
                : MediaRecorder.isTypeSupported('audio/ogg;codecs=opus')
                ? 'audio/ogg;codecs=opus'
                : '';
            
            console.log('Selected mimeType:', mimeType || 'browser default');
            
            const mediaRecorder = mimeType 
                ? new MediaRecorder(audioStream, { mimeType })
                : new MediaRecorder(audioStream);
            
            // ì‹¤ì œ ì‚¬ìš©ë˜ëŠ” mimeType í™•ì¸
            console.log('MediaRecorder actual mimeType:', mediaRecorder.mimeType);
            
            audioMediaRecorderRef.current = mediaRecorder;
            
            mediaRecorder.ondataavailable = async (event) => {
                // Blob íƒ€ì… ë¡œê¹…
                console.log('Received audio chunk - Blob type:', event.data.type, ', size:', event.data.size);
                
                // ë¹ˆ ì²­í¬ ìŠ¤í‚µ
                if (event.data.size === 0) {
                    console.log('Skipping empty audio chunk');
                    return;
                }
                
                if (audioWsRef.current && audioWsRef.current.readyState === WebSocket.OPEN) {
                    try {
                        // Blobì„ arrayBufferë¡œ ì½ê¸°
                        const buf = await event.data.arrayBuffer();
                        let bytes = new Uint8Array(buf);
                        
                        // í—¤ë” ê°ì§€ (WebM: 1A 45 DF A3, Ogg: 4F 67 67 53)
                        const isWebm = bytes.length >= 4 && 
                            bytes[0] === 0x1a && bytes[1] === 0x45 && 
                            bytes[2] === 0xdf && bytes[3] === 0xa3;
                        const isOgg = bytes.length >= 4 && 
                            bytes[0] === 0x4f && bytes[1] === 0x67 && 
                            bytes[2] === 0x67 && bytes[3] === 0x53;
                        
                        // ë””ë²„ê·¸: ì• 4ë°”ì´íŠ¸ í™•ì¸
                        const headerBytes = [...bytes.slice(0, 4)].map(x => x.toString(16).toUpperCase().padStart(2, '0'));
                        console.log('Audio chunk header (first 4 bytes):', headerBytes, isWebm ? '(WebM)' : isOgg ? '(Ogg)' : '(No header)');
                        
                        // ì²« ì²­í¬(í—¤ë” í¬í•¨)ë¥¼ ìºì‹œ
                        if (!audioHeaderRef.current && isWebm) {
                            audioHeaderRef.current = bytes;
                            console.log('âœ“ Cached WebM header for subsequent chunks');
                        }
                        
                        // í—¤ë” ì—†ëŠ” í›„ì† ì²­í¬ì— ìºì‹œëœ í—¤ë” ë¶™ì´ê¸°
                        if (!isWebm && !isOgg && audioHeaderRef.current) {
                            const combined = new Uint8Array(audioHeaderRef.current.length + bytes.length);
                            combined.set(audioHeaderRef.current, 0);
                            combined.set(bytes, audioHeaderRef.current.length);
                            bytes = combined;
                            console.log('âœ“ Prepended cached header to chunk (new size:', bytes.byteLength, 'bytes)');
                        }
                        
                        // 1. JSON ë©”íƒ€ ì „ì†¡ (text frame)
                        const metadata = {
                            type: 'audio_chunk',
                            roomId: roomId,
                            userId: userId,
                            userName: userName,
                            seq: audioSeqRef.current++,
                            sampleRate: 48000,
                            channels: 1,
                            codec: 'opus',
                            startedAt: Date.now()
                        };
                        
                        audioWsRef.current.send(JSON.stringify(metadata));
                        console.log('âœ“ Sent audio metadata (seq:', metadata.seq, ', roomId:', metadata.roomId, ', userId:', metadata.userId, ', userName:', metadata.userName, ')');
                        
                        // 2. ë°”ì´ë„ˆë¦¬ ë°ì´í„° ì „ì†¡ (binary frame) - Uint8Array ê·¸ëŒ€ë¡œ
                        audioWsRef.current.send(bytes);
                        console.log('âœ“ Sent audio binary chunk:', bytes.byteLength, 'bytes');
                    } catch (error) {
                        console.error('Failed to send audio chunk:', error);
                    }
                }
            };
            
            mediaRecorder.start(1000); // 1ì´ˆ ì²­í¬ (timeslice)
            console.log('âœ“ Audio recording started for AI processing');
            console.log('  - roomId:', roomId);
            console.log('  - userId:', userId);
            console.log('  - userName:', userName);
            console.log('  - Stream:', audioStream.id);
            console.log('  - Audio tracks:', audioStream.getAudioTracks().length);
        } catch (error) {
            console.error('Failed to start audio recording:', error);
        }
    };
    
    const stopAudioRecording = () => {
        if (audioMediaRecorderRef.current && audioMediaRecorderRef.current.state !== 'inactive') {
            audioMediaRecorderRef.current.stop();
            audioMediaRecorderRef.current = null;
            console.log('Audio recording stopped');
        }
        // í—¤ë” ìºì‹œ ì´ˆê¸°í™”
        audioHeaderRef.current = null;
    };
    
    const handleSeparatedAudio = (message: any) => {
        console.log('Received separated audio:', message);
        
        // Base64 ë°ì´í„°ë¥¼ Blobìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì¬ìƒ/ì‹œê°í™”
        const { fromUserId, seq, data, transcriptChunk } = message;
        
        if (data) {
            Object.entries(data).forEach(([src, base64Data]: [string, any]) => {
                try {
                    // Base64ë¥¼ Blobìœ¼ë¡œ ë³€í™˜
                    const binaryString = atob(base64Data);
                    const bytes = new Uint8Array(binaryString.length);
                    for (let i = 0; i < binaryString.length; i++) {
                        bytes[i] = binaryString.charCodeAt(i);
                    }
                    const blob = new Blob([bytes], { type: 'audio/wav' });
                    const url = URL.createObjectURL(blob);
                    
                    console.log(`Separated audio ${src} from user ${fromUserId}, seq ${seq}:`, url);
                    
                    // ìƒíƒœì— ì €ì¥ (ë‚˜ì¤‘ì— ì¬ìƒ/ì‹œê°í™” ê°€ëŠ¥)
                    setSeparatedAudioData(prev => ({
                        ...prev,
                        [fromUserId]: [...(prev[fromUserId] || []), { src, data: url }]
                    }));
                    
                    // ìë™ ì¬ìƒ (ì„ íƒì‚¬í•­)
                    // const audio = new Audio(url);
                    // audio.play();
                } catch (error) {
                    console.error(`Failed to process separated audio ${src}:`, error);
                }
            });
        }
        
        if (transcriptChunk) {
            console.log('Transcript chunk:', transcriptChunk);
            // STT ê²°ê³¼ë¥¼ ì±„íŒ… ë©”ì‹œì§€ë¡œ í‘œì‹œ
            setMessages(prev => [...prev, { 
                name: 'STT', 
                text: transcriptChunk 
            }]);
        }
    };
    
    const getSummary = async (language: string = 'ko', maxSentences: number = 5) => {
        try {
            if (!roomId) {
                throw new Error('Room ID is required');
            }
            
            // 1. ë°±ì—”ë“œ STTì—ì„œ íšŒì˜ë¡ ê°€ì ¸ì˜¤ê¸°
            const { getRealAiBaseUrl } = await import('../lib/envCheck');
            const baseUrl = getRealAiBaseUrl();
            
            const transcriptResponse = await fetch(`${baseUrl}/stt/transcript/${roomId}`);
            
            if (!transcriptResponse.ok) {
                throw new Error(`Failed to get transcript: ${transcriptResponse.statusText}`);
            }
            
            const transcriptData = await transcriptResponse.json();
            const fullTranscript = transcriptData.fullTranscript;
            
            if (!fullTranscript || fullTranscript.trim().length === 0) {
                throw new Error('íšŒì˜ë¡ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤');
            }
            
            console.log('ğŸ“ íšŒì˜ë¡ ê°€ì ¸ì˜´:', fullTranscript.substring(0, 200) + '...');
            
            // 2. ìš”ì•½ ìš”ì²­
            const summaryResponse = await fetch(`${baseUrl}/summaries`, {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                },
                body: JSON.stringify({ 
                    transcript: fullTranscript,
                    max_keywords: 10,
                    hf_min_length: 30,
                    hf_max_length: 60
                })
            });
            
            if (!summaryResponse.ok) {
                throw new Error(`Failed to get summary: ${summaryResponse.statusText}`);
            }
            
            const result = await summaryResponse.json();
            console.log('âœ… ìš”ì•½ ì™„ë£Œ:', result);
            
            return {
                keywords: result.keywords || [],
                huggingfaceSummary: result.huggingface_summary || null,
                chatgptSummary: result.chatgpt_summary || null,
                geminiSummary: result.gemini_summary || null
            };
        } catch (error) {
            console.error('ìš”ì•½ ìƒì„± ì‹¤íŒ¨:', error);
            throw error;
        }
    };

    const startRecording = async () => {
        try {
            // AudioContext ìƒì„±
            const audioContext = new AudioContext();
            audioContextRef.current = audioContext;

            // Destination ìƒì„± (ëª¨ë“  ì˜¤ë””ì˜¤ë¥¼ ë¯¹ì‹±í•  ê³³)
            const destination = audioContext.createMediaStreamDestination();

            // ë¡œì»¬ ë§ˆì´í¬ ì˜¤ë””ì˜¤ ì¶”ê°€
            if (localStreamRef.current) {
                const localAudioTrack = localStreamRef.current.getAudioTracks()[0];
                if (localAudioTrack) {
                    const localSource = audioContext.createMediaStreamSource(
                        new MediaStream([localAudioTrack])
                    );
                    localSource.connect(destination);
                    console.log('Local audio added to recording');
                }
            }

            // ëª¨ë“  ì›ê²© ì˜¤ë””ì˜¤ ì¶”ê°€
            Object.entries(remoteStreams).forEach(([userId, stream]) => {
                const audioTracks = stream.getAudioTracks();
                if (audioTracks.length > 0) {
                    const remoteSource = audioContext.createMediaStreamSource(
                        new MediaStream([audioTracks[0]])
                    );
                    remoteSource.connect(destination);
                    console.log(`Remote audio from ${userId} added to recording`);
                }
            });

            // ë¯¹ì‹±ëœ ìŠ¤íŠ¸ë¦¼ ì €ì¥
            mixedStreamRef.current = destination.stream;

            // MediaRecorder ìƒì„±
            const mediaRecorder = new MediaRecorder(destination.stream, {
                mimeType: 'audio/webm;codecs=opus'
            });

            recordedChunksRef.current = [];

            mediaRecorder.ondataavailable = (event) => {
                if (event.data.size > 0) {
                    recordedChunksRef.current.push(event.data);
                }
            };

            mediaRecorder.onstop = () => {
                console.log('Recording stopped, chunks:', recordedChunksRef.current.length);
                // ë…¹ìŒ ë°ì´í„°ëŠ” ì˜¤ë””ì˜¤ WebSocketì„ í†µí•´ ì‹¤ì‹œê°„ìœ¼ë¡œ ì „ì†¡ë¨
                recordedChunksRef.current = [];
            };

            mediaRecorder.start(1000); // 1ì´ˆë§ˆë‹¤ ë°ì´í„° ìˆ˜ì§‘
            mediaRecorderRef.current = mediaRecorder;
            setIsRecording(true);

            console.log('Recording started');
        } catch (error) {
            console.error('Failed to start recording:', error);
            throw error;
        }
    };

    const stopRecording = () => {
        if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
            mediaRecorderRef.current.stop();
            setIsRecording(false);
            console.log('Stopping recording...');
        }

        // AudioContext ì •ë¦¬
        if (audioContextRef.current) {
            audioContextRef.current.close();
            audioContextRef.current = null;
        }

        mixedStreamRef.current = null;
    };

    const handleResize = (e: React.MouseEvent) => {
        const startX = e.clientX;
        const startWidth = chatWidth;

        const onMouseMove = (moveEvent: MouseEvent) => {
            const newWidth = startWidth + (moveEvent.clientX - startX);
            if (newWidth >= 200 && newWidth <= 800) setChatWidth(newWidth);
        };
        const onMouseUp = () => {
            document.removeEventListener("mousemove", onMouseMove);
            document.removeEventListener("mouseup", onMouseUp);
        };
        document.addEventListener("mousemove", onMouseMove);
        document.addEventListener("mouseup", onMouseUp);
    };

    const handleSendMessage = async () => {
        if (!message.trim()) return;
        
        const messageText = message;
        setMessage(""); // ë¨¼ì € ì…ë ¥ì°½ ë¹„ìš°ê¸°
        
        try {
            // WebSocketìœ¼ë¡œ ì±„íŒ… ë©”ì‹œì§€ ì „ì†¡ (ì„œë²„ ìŠ¤í™ì— ë§ì¶° message í•„ë“œ ì‚¬ìš©)
            sendWebSocketMessage({
                type: 'chat',
                message: messageText
            });
        } catch (error) {
            console.error('Failed to send message:', error);
        }
    };

    const handleKeyDown = (e: React.KeyboardEvent<HTMLInputElement>) => {
        if (e.key === "Enter" && !e.nativeEvent.isComposing) {
            e.preventDefault();
            handleSendMessage();
        }
    };
    
    // ë°±ì—”ë“œ STT: ì˜¤ë””ì˜¤ ìº¡ì²˜ ë° ì „ì†¡
    const startAudioCapture = async (userId: string, userName: string, stream: MediaStream, currentRoomId: string) => {
        try {
            const audioTrack = stream.getAudioTracks()[0];
            if (!audioTrack) {
                console.error('âŒ No audio track found');
                return;
            }
            
            console.log('ğŸ¤ Audio track:', {
                label: audioTrack.label,
                enabled: audioTrack.enabled,
                muted: audioTrack.muted,
                readyState: audioTrack.readyState,
                roomId: currentRoomId
            });
            
            const audioStream = new MediaStream([audioTrack]);
            const mediaRecorder = new MediaRecorder(audioStream, {
                mimeType: 'audio/webm;codecs=opus'
            });
            
            let chunks: Blob[] = [];
            
            mediaRecorder.onstart = () => {
                console.log('âœ… MediaRecorder started for', userName, 'in room', currentRoomId);
                chunks = [];
            };
            
            mediaRecorder.ondataavailable = (event) => {
                if (event.data.size > 0) {
                    chunks.push(event.data);
                    console.log('ğŸ“¦ Chunk collected:', event.data.size, 'bytes');
                }
            };
            
            mediaRecorder.onstop = async () => {
                if (chunks.length === 0 || !currentRoomId) {
                    console.log('âš ï¸ No chunks or roomId, skipping');
                    return;
                }
                
                // ì™„ì „í•œ Blob ìƒì„±
                const completeBlob = new Blob(chunks, { type: 'audio/webm' });
                console.log('ğŸ¬ Complete audio:', completeBlob.size, 'bytes');
                
                // í¬ê¸° ê²€ì¦: ìµœì†Œ 10KB (ì•½ 1ì´ˆ ì´ìƒì˜ ì˜¤ë””ì˜¤)
                const MIN_AUDIO_SIZE = 10000; // 10KB
                if (completeBlob.size < MIN_AUDIO_SIZE) {
                    console.log(`âš ï¸ Audio too small (${completeBlob.size} bytes < ${MIN_AUDIO_SIZE} bytes), skipping`);
                    chunks = [];
                    return;
                }
                
                // ì˜ˆìƒ ì˜¤ë””ì˜¤ ê¸¸ì´ ê³„ì‚° (ëŒ€ëµì )
                // WebM Opus: ~16KB/sec (ë¹„íŠ¸ë ˆì´íŠ¸ì— ë”°ë¼ ë‹¤ë¦„)
                const estimatedDuration = completeBlob.size / 16000; // ì´ˆ ë‹¨ìœ„
                if (estimatedDuration < 1.0) {
                    console.log(`âš ï¸ Audio too short (${estimatedDuration.toFixed(2)}s < 1.0s), skipping`);
                    chunks = [];
                    return;
                }
                
                console.log(`âœ… Audio validation passed: ${completeBlob.size} bytes (~${estimatedDuration.toFixed(2)}s)`);
                
                const { getRealAiBaseUrl } = await import('../lib/envCheck');
                const baseUrl = getRealAiBaseUrl();
                
                const formData = new FormData();
                formData.append('audio', completeBlob, 'audio.webm');
                formData.append('userId', userId);
                formData.append('userName', userName);
                formData.append('roomId', currentRoomId);
                
                console.log(`ğŸ“¤ STT ì „ì†¡ ì¤‘:`, {
                    url: `${baseUrl}/stt/stream`,
                    userId,
                    userName,
                    roomId: currentRoomId,
                    audioSize: completeBlob.size,
                    estimatedDuration: `${estimatedDuration.toFixed(2)}s`
                });
                
                try {
                    const res = await fetch(`${baseUrl}/stt/stream`, {
                        method: 'POST',
                        body: formData
                    });
                    const data = await res.json();
                    console.log('âœ… STT ì‘ë‹µ:', data);
                    if (data.transcript) {
                        console.log(`ğŸ—£ï¸ ${userName}: ${data.transcript}`);
                    }
                } catch (err) {
                    console.error('âŒ STT ì „ì†¡ ì‹¤íŒ¨:', err);
                }
                
                chunks = [];
            };
            
            // 30ì´ˆë§ˆë‹¤ stop â†’ start ë°˜ë³µ
            const recordingInterval = setInterval(() => {
                if (mediaRecorder.state === 'recording') {
                    mediaRecorder.stop();
                    setTimeout(() => {
                        if (mediaRecorder.state === 'inactive') {
                            chunks = [];
                            mediaRecorder.start();
                        }
                    }, 100);
                }
            }, 30000); // 30ì´ˆ
            
            // refì— ì €ì¥í•˜ì—¬ ë‚˜ì¤‘ì— ì œì–´ ê°€ëŠ¥í•˜ë„ë¡
            audioRecordersRef.current[userId] = {
                recorder: mediaRecorder,
                intervalId: recordingInterval
            };
            
            mediaRecorder.start();
            console.log(`ğŸ™ï¸ ${userName} ì˜¤ë””ì˜¤ ìº¡ì²˜ ì‹œì‘`);
        } catch (error) {
            console.error('ì˜¤ë””ì˜¤ ìº¡ì²˜ ì‹¤íŒ¨:', error);
        }
    };

    useEffect(() => {
        return () => {
            leaveRoom();
        };
    }, []);

    return {
        showParticipants,
        setShowParticipants,
        chatWidth,
        chatScrollRef,
        videoRef,
        message,
        setMessage,
        messages,
        participants,
        remoteStreams,
        selectedParticipant,
        setSelectedParticipant,
        localStream: localStreamRef.current,
        roomId,
        isConnected,
        connectionStatus,
        isScreenSharing,
        isRecording,
        separatedAudioData,
        handleResize,
        handleSendMessage,
        handleKeyDown,
        createRoom,
        joinRoom,
        leaveRoom,
        listRooms,
        findOrCreateTeamRoom,
        toggleCamera,
        toggleMicrophone,
        startScreenShare,
        stopScreenShare,
        startRecording,
        stopRecording,
        getSummary,
    };
}